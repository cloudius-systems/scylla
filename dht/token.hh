/*
 * Copyright (C) 2020-present ScyllaDB
 */

/*
 * This file is part of Scylla.
 *
 * Scylla is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * Scylla is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with Scylla.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "bytes.hh"
#include "utils/managed_bytes.hh"
#include "types.hh"

#include <seastar/net/byteorder.hh>
#include <fmt/format.h>
#include <array>
#include <functional>
#include <utility>
#include <compare>

namespace dht {

class token;

// Once upon a time, we had special before_all_keys and after_all_keys tokens,
// which were used to represent unbounded intervals. But our interval types
// have their own ways to do that, so those special tokens were removed as
// redundant.
//
// However, dht::token is a type used in IDL, so we need to handle token_kind
// in RPC for backward compatibility. token_kind should only be used on the
// RPC boundary.
enum class token_kind {
    before_all_keys,
    key,
    after_all_keys,
};

struct legacy_token {
    token_kind _kind;
    int64_t _data;

    legacy_token(token_kind k, int64_t d) : _kind(k), _data(d) {}

    // IDL constructor.
    legacy_token(token_kind k, const bytes& b) : _kind(k) {
        switch (k) {
        case token_kind::before_all_keys:
            _data = std::numeric_limits<int64_t>::min();
            break;
        case token_kind::after_all_keys:
            _data = std::numeric_limits<int64_t>::max();
            break;
        default:
            if (b.size() != sizeof(_data)) {
                throw std::runtime_error(fmt::format("Wrong token bytes size: expected {} but got {}", sizeof(_data), b.size()));
            }
            _data = net::ntoh(read_unaligned<int64_t>(b.begin()));
        }
    }

    bytes data() const {
        bytes b(bytes::initialized_later(), sizeof(_data));
        write_unaligned<int64_t>(b.begin(), net::hton(_data));
        return b;
    }
};

class token {
    // INT64_MIN is a reserved token, and no key is partitioned into it.
    // If a token with value INT64_MIN is generated by the hashing algorithm,
    // the result is coerced into INT64_MAX.
    // (So INT64_MAX is twice as likely as every other token.)
    //
    // This design was chosen by Cassandra, because it allows for convenient
    // representation of any interval of valid tokens with just a (start, end]
    // pair. If all tokens were valid, this would be impossible (because there
    // would be (2**64 + 1)**2 intervals, and only (2**64)**2 pairs.)
    static inline int64_t normalize(int64_t t) {
        return t == std::numeric_limits<int64_t>::min() ? std::numeric_limits<int64_t>::max() : t;
    }
public:
    using kind = token_kind;
    int64_t _data;

    token() : _data(std::numeric_limits<int64_t>::min()) {}

    token(token_kind k, int64_t d) {
        switch (k) {
        case token_kind::before_all_keys:
            _data = std::numeric_limits<int64_t>::min();
            break;
        case token_kind::after_all_keys:
            // Should not be happening. Throw?
            _data = std::numeric_limits<int64_t>::max();
            break;
        default:
            _data = normalize(d);
            break;
        }
    }

    token(token_kind k, bytes_view b) {
        switch (k) {
        case token_kind::before_all_keys:
            _data = std::numeric_limits<int64_t>::min();
            break;
        case token_kind::after_all_keys:
            // Should not be happening. Throw?
            _data = std::numeric_limits<int64_t>::max();
            break;
        default:
            if (b.size() != sizeof(_data)) {
                throw std::runtime_error(fmt::format("Wrong token bytes size: expected {} but got {}", sizeof(_data), b.size()));
            }
            _data = net::ntoh(read_unaligned<int64_t>(b.begin()));
        }
    }

    token(legacy_token t) : _data(t._data) {}
    legacy_token legacy() const {
        if (is_minimum()) {
            return legacy_token(token_kind::before_all_keys, _data);
        }
        return legacy_token(token_kind::key, _data);
    }

    bool is_minimum() const noexcept {
        return _data == std::numeric_limits<int64_t>::min();
    }

    bool is_maximum() const noexcept {
        return false;
    }

    size_t external_memory_usage() const {
        return 0;
    }

    size_t memory_usage() const {
        return sizeof(token);
    }

    bytes data() const {
        bytes b(bytes::initialized_later(), sizeof(_data));
        write_unaligned<int64_t>(b.begin(), net::hton(_data));
        return b;
    }

    /**
     * @return a string representation of this token
     */
    sstring to_sstring() const;

    /**
     * Calculate a token representing the approximate "middle" of the given
     * range.
     *
     * @return The approximate midpoint between left and right.
     */
    static token midpoint(const token& left, const token& right);

    /**
     * @return a randomly generated token
     */
    static token get_random_token();

    /**
     * @return a token from string representation
     */
    static dht::token from_sstring(const sstring& t);

    /**
     * @return a token from its byte representation
     */
    static dht::token from_bytes(bytes_view bytes);

    /**
     * Returns int64_t representation of the token
     */
    int64_t to_int64() const noexcept {
        return _data;
    }

    /**
     * Creates token from its int64_t representation
     */
    static dht::token from_int64(int64_t);

    /**
     * Calculate the deltas between tokens in the ring in order to compare
     *  relative sizes.
     *
     * @param sortedtokens a sorted List of tokens
     * @return the mapping from 'token' to 'percentage of the ring owned by that token'.
     */
    static std::map<token, float> describe_ownership(const std::vector<token>& sorted_tokens);

    static data_type get_token_validator();
};

static inline std::strong_ordering tri_compare_raw(const int64_t l1, const int64_t l2) noexcept {
    return l1 <=> l2;
}

template <typename T>
concept TokenCarrier = requires (const T& v) {
    { v.token() } noexcept -> std::same_as<const token&>;
};

struct raw_token_less_comparator {
    bool operator()(const int64_t k1, const int64_t k2) const noexcept {
        return dht::tri_compare_raw(k1, k2) < 0;
    }

    template <typename Key>
    requires TokenCarrier<Key>
    bool operator()(const Key& k1, const int64_t k2) const noexcept {
        return dht::tri_compare_raw(k1.token().to_int64(), k2) < 0;
    }

    template <typename Key>
    requires TokenCarrier<Key>
    bool operator()(const int64_t k1, const Key& k2) const noexcept {
        return dht::tri_compare_raw(k1, k2.token().to_int64()) < 0;
    }

    template <typename Key>
    requires TokenCarrier<Key>
    int64_t simplify_key(const Key& k) const noexcept {
        return k.token().to_int64();
    }

    int64_t simplify_key(int64_t k) const noexcept {
        return k;
    }
};

const token& minimum_token() noexcept;
const token& greatest_token() noexcept;

inline std::strong_ordering operator<=>(const token& t1, const token& t2) { return t1._data <=> t2._data; }
inline bool operator==(const token& t1, const token& t2) { return t1._data == t2._data; }
inline std::strong_ordering tri_compare(const token& t1, const token& t2) { return t1 <=> t2; }

std::ostream& operator<<(std::ostream& out, const token& t);

} // namespace dht
